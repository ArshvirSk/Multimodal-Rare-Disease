# Multimodal Machine Learning Framework for Rare Genetic Disease Diagnosis

A deep learning framework that combines **facial phenotype analysis** and **clinical narrative understanding** for automated rare genetic disease classification.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸ¯ Overview

This framework implements a multimodal AI diagnostic system that fuses:

- **CNN-based facial encoder** (ResNet50/EfficientNet-B0) for facial dysmorphism detection
- **BioBERT/ClinicalBERT text encoder** for clinical narrative understanding
- **Attention-based fusion** for combining modalities
- **Statistical validation** using Chi-square tests

### Supported Syndromes

The model is trained to classify 10 rare genetic syndromes:

1. Cornelia de Lange Syndrome
2. Williams-Beuren Syndrome
3. Noonan Syndrome
4. Kabuki Syndrome
5. KBG Syndrome
6. Angelman Syndrome
7. Rubinstein-Taybi Syndrome
8. Smith-Magenis Syndrome
9. Nicolaides-Baraitser Syndrome
10. 22q11.2 Deletion Syndrome

## ğŸ“ Project Structure

```
multimodal-rare-disease/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ FGDD/                    # FGDD phenotype data (Figshare)
â”‚   â”œâ”€â”€ orphadata/               # Orphadata XML files
â”‚   â”‚   â”œâ”€â”€ orphadata_diseases.xml
â”‚   â”‚   â”œâ”€â”€ orphadata_phenotypes.xml
â”‚   â”‚   â””â”€â”€ orphadata_genes.xml
â”‚   â””â”€â”€ hpo/                     # Human Phenotype Ontology
â”‚       â”œâ”€â”€ hp.obo
â”‚       â””â”€â”€ phenotype.hpoa
â”œâ”€â”€ PDIDB/                       # Synthetic facial image generator
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                # Configuration and hyperparameters
â”‚   â”œâ”€â”€ image_dataset_loader.py  # Image preprocessing + augmentation
â”‚   â”œâ”€â”€ text_dataset_loader.py   # Clinical text processing
â”‚   â”œâ”€â”€ cnn_encoder.py           # ResNet50/EfficientNet encoder
â”‚   â”œâ”€â”€ text_encoder.py          # BioBERT/ClinicalBERT encoder
â”‚   â”œâ”€â”€ fusion_model.py          # Attention-based fusion
â”‚   â”œâ”€â”€ multimodal_classifier.py # Complete multimodal model
â”‚   â”œâ”€â”€ train.py                 # Training pipeline
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation metrics
â”‚   â”œâ”€â”€ predict.py               # Inference pipeline
â”‚   â””â”€â”€ chi_square_test.py       # Statistical validation
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ explainability.ipynb     # Grad-CAM + attention visualization
â”œâ”€â”€ results/
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš¡ Quick Start

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/multimodal-rare-disease.git
cd multimodal-rare-disease

# Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Train the model (with data augmentation for small datasets)
python run_training.py --image-dirs data/images_augmented --epochs 60

# Make a prediction on a new image
python predict.py --image path/to/face.jpg
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (recommended)
- 16GB+ RAM

### Installation

```bash
# Clone or navigate to the project
cd multimodal-rare-disease

# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/Mac)
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Download Pretrained Models

The framework automatically downloads pretrained models on first run:

- ResNet50 (ImageNet weights)
- BioBERT (dmis-lab/biobert-base-cased-v1.2)

## ğŸ“Š Data Sources

### Facial Images (PDIDB)

Synthetic facial images generated using StyleGAN3, trained on GestaltMatcher Database.

Repository: https://github.com/WGLab/PDIDB

### Clinical Text (Orphadata + HPO)

- **Orphadata**: Rare disease descriptions and phenotype associations
- **HPO**: Human Phenotype Ontology standard vocabulary

## ğŸ‹ï¸ Training

### Quick Smoke Test

```bash
python -m src.train --mode multimodal --smoke_test
```

### Full Training

```bash
# Multimodal (image + text)
python -m src.train --mode multimodal --epochs 100

# Image-only baseline
python -m src.train --mode image_only --epochs 100

# Text-only baseline
python -m src.train --mode text_only --epochs 100
```

### Training Options

```
--mode          Training mode: multimodal, image_only, text_only
--epochs        Number of training epochs (default: 100)
--batch_size    Batch size (default: 16)
--lr            Learning rate (default: 1e-4)
--device        Device: cuda or cpu
```

## ğŸ“ˆ Evaluation

```bash
python -m src.evaluate --checkpoint checkpoints/multimodal_best.pt --mode multimodal
```

### Metrics Computed

- Accuracy, Precision, Recall, F1-score
- Per-class metrics
- Confusion matrix
- ROC-AUC curves

## ğŸ”¬ Statistical Validation

Compare multimodal vs unimodal using Chi-square test:

```bash
# Run demo with synthetic data
python -m src.chi_square_test --demo

# Run on real predictions
python -m src.chi_square_test --predictions_dir results
```

**Hypothesis Testing:**

- H0: Multimodal and unimodal have same performance
- H1: Multimodal outperforms unimodal (p < 0.05)

## ğŸ”® Inference

```bash
python -m src.predict \
    --image path/to/face.jpg \
    --text "Patient presents with hypertelorism, seizures, and delayed speech." \
    --checkpoint checkpoints/multimodal_best.pt \
    --output prediction.json
```

### Output Example

```json
{
  "predictions": [
    {
      "syndrome": "Angelman Syndrome",
      "confidence": 0.85,
      "probability_percent": 85.0
    },
    {
      "syndrome": "Williams-Beuren Syndrome",
      "confidence": 0.08,
      "probability_percent": 8.0
    }
  ],
  "top_prediction": {
    "syndrome": "Angelman Syndrome",
    "confidence": 0.85
  }
}
```

## ğŸ§  Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Facial Image   â”‚     â”‚ Clinical Text   â”‚
â”‚  (224Ã—224Ã—3)    â”‚     â”‚ (max 128 tokens)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CNN Encoder    â”‚     â”‚  Text Encoder   â”‚
â”‚  (ResNet50)     â”‚     â”‚  (BioBERT)      â”‚
â”‚  â†’ 512-d        â”‚     â”‚  â†’ 768-d        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Attention Fusion     â”‚
         â”‚  Cross-modal Attentionâ”‚
         â”‚  â†’ 512-d              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Classification Head  â”‚
         â”‚  FC â†’ ReLU â†’ Dropout  â”‚
         â”‚  â†’ N syndromes        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¨ Explainability

The framework includes explainability features:

1. **Grad-CAM**: Visualize which facial regions influence predictions
2. **Attention Weights**: Understand which clinical terms are important
3. **Cross-modal Attention**: See how image and text modalities interact

See `notebooks/explainability.ipynb` for interactive visualization.

## ğŸ“‹ Configuration

All hyperparameters are centralized in `src/config.py`:

```python
# Key configurations
config.cnn_encoder.backbone = "resnet50"  # or "efficientnet_b0"
config.text_encoder.model_name = "dmis-lab/biobert-base-cased-v1.2"
config.fusion.fusion_type = "attention"  # or "concatenation", "gated"
config.training.learning_rate = 1e-4
config.training.batch_size = 16
config.training.num_epochs = 100
```

## ğŸ“š References

### Datasets

- [PDIDB - Phenotype Disease Image Database](https://github.com/WGLab/PDIDB)
- [FGDD - Facial Gestalt Disease Database](https://doi.org/10.6084/m9.figshare.28516604)
- [Orphadata](https://www.orphadata.com/)
- [Human Phenotype Ontology](https://hpo.jax.org/)

### Models

- [BioBERT](https://github.com/dmis-lab/biobert)
- [ResNet](https://arxiv.org/abs/1512.03385)

## ğŸ“„ License

This project is licensed under the MIT License.

## âš ï¸ Disclaimer

This is a research tool for educational purposes. **Do not use for clinical diagnosis.**
All predictions should be validated by qualified medical professionals.
